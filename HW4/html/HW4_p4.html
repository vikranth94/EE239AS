
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>HW4_p4</title><meta name="generator" content="MATLAB 9.0"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-05-17"><meta name="DC.source" content="HW4_p4.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#2">Problem 4: Real Neural Data</a></li><li><a href="#3">Part A: ML Parameters</a></li><li><a href="#4">Part A: Model(i) Classification</a></li><li><a href="#5">Part B: Model (ii) Classification</a></li><li><a href="#6">Part C: Model (ii) Classification (Removing Neurons)</a></li><li><a href="#7">Part D: Naive Bayes</a></li></ul></div><pre class="codeinput"><span class="comment">% EE239AS Homework 4</span>

clc
clear
close <span class="string">all</span>
</pre><h2>Problem 4: Real Neural Data<a name="2"></a></h2><pre class="codeinput">ps4_data = importdata(<span class="string">'ps4_realdata.mat'</span>);
</pre><h2>Part A: ML Parameters<a name="3"></a></h2><pre class="codeinput">train_data = ps4_data.train_trial;
test_data = ps4_data.test_trial;
<span class="comment">% since sizes of test and train data sets are the same, extract number of</span>
<span class="comment">% spikes in the same loop</span>

n_neurons = 97;           <span class="comment">% number of neurons per trial</span>
n_class = size(train_data,2);       <span class="comment">% number of classes</span>
n_trial = size(train_data,1);       <span class="comment">% number of trials</span>
n_spikes_train = cell(1, n_class);
<span class="comment">% number of spikes per spike train (train data)</span>
n_spikes_test = n_spikes_train;
<span class="comment">% number of spikes per spike train (test data)</span>

<span class="comment">% sum the total number of spikes per trial from spike train data</span>

<span class="keyword">for</span> i = 1:n_trial
    <span class="keyword">for</span> j = 1:n_class
        n_spikes_train{1,j} = [n_spikes_train{1,j}, sum(train_data(i,j).spikes(:,351:550),2)];
        n_spikes_test{1,j} = [n_spikes_test{1,j}, sum(test_data(i,j).spikes(:,351:550),2)];
        <span class="comment">% only include the spikes in the middle of the trial due to</span>
        <span class="comment">% experimental procedure</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><h2>Part A: Model(i) Classification<a name="4"></a></h2><pre class="codeinput"><span class="comment">% Model (i) Gaussian, Shared Covariance</span>

N_k = n_trial;      <span class="comment">% number of neurons per class is equal for all classes</span>
N = N_k*n_class;    <span class="comment">% total number of neurons</span>
P_Ck = N_k/(n_class*N_k);
<span class="comment">% prior probabilities of each class (equal for all classes)</span>

mu_i = zeros(n_neurons, n_class);
S_k_i = cell(1, n_class);
sigma_i = zeros(n_neurons, n_neurons);
<span class="comment">% preallocate ML parameter vectors</span>

<span class="comment">% calculating ML parameters for classification boundaries</span>
<span class="keyword">for</span> i = 1:n_class
    mu_i(:,i) = 1/(N_k)*sum(n_spikes_train{1,i},2);
    <span class="comment">% calculate mean using results of problem 1</span>
    cov_trial_i = zeros(n_neurons, n_neurons);
    <span class="comment">% (x-mu)*(x-mu)' matrices for each trial</span>
    <span class="keyword">for</span> j = 1:n_trial
        cov_trial_i = cov_trial_i + (n_spikes_train{1,i}(:,j)-mu_i(:,i))*(n_spikes_train{1,i}(:,j)-mu_i(:,i))';
        <span class="comment">% sum the (x-mu)*(x-mu)' matrices for each trial</span>
    <span class="keyword">end</span>

    S_k_i{i} = 1/N_k * cov_trial_i;
    <span class="comment">% calculate the S_k for each class and store into cell</span>
    sigma_i = sigma_i + N_k/N * S_k_i{i};
    <span class="comment">% calculate sigma (weighted sum of S_k)</span>
<span class="keyword">end</span>

k = zeros(n_trial, n_class);
errors = zeros(1,n_class);
<span class="comment">% preallocate the class prediction and error vectors</span>

<span class="comment">% classification of test data</span>
<span class="keyword">for</span> n = 1:n_class
    xy = n_spikes_test{n}';
    <span class="comment">% matrix of neuron firing rates across all trials (97x91) transposed</span>
    <span class="keyword">for</span> j = 1:n_trial
        <span class="keyword">for</span> i = 1:n_class
            k(j,i) = log(P_Ck)+ mu_i(:,i)'*inv(sigma_i)*xy(j,:)'-0.5*mu_i(:,i)'<span class="keyword">...</span>
                *inv(sigma_i)*mu_i(:,i);
            <span class="comment">% decision function for each neuron a_k(x)</span>
        <span class="keyword">end</span>
    <span class="keyword">end</span>
    [m,idx] = max(k, [], 2);
    <span class="comment">% take the maximum of this to decide which class the neuron belongs in</span>
    errors(n)= length(idx(idx~=n));
    <span class="comment">% number of incorrect classifications (not matching the specified reach</span>
    <span class="comment">% angle)</span>
<span class="keyword">end</span>
accuracy = (1-sum(errors)/(n_trial*n_class))*100;
fprintf(<span class="string">'Model (i) Gaussian, Shared Covariance Accuracy in percentage:'</span>)
disp(accuracy)
<span class="comment">% calculate accuracy of classification</span>
</pre><pre class="codeoutput">Model (i) Gaussian, Shared Covariance Accuracy in percentage:   96.0165

</pre><h2>Part B: Model (ii) Classification<a name="5"></a></h2><pre class="codeinput"><span class="comment">% Model (ii) Gaussian, Class Specific Covariance</span>

<span class="comment">% The code above was run using the ML parameters for Model (ii) (same mu,</span>
<span class="comment">% and class-specific sigmas). However, the error:</span>

<span class="comment">% Matrix is close to singular or badly scaled. Results may be inaccurate.</span>

<span class="comment">% occurs. This happens because there are neurons that do not fire very much</span>
<span class="comment">% across all the trials, causing the covariance matrix to be close to</span>
<span class="comment">% singular (due to the determinant being very close to zero). When the</span>
<span class="comment">% covariance matrix is singular, it is not invertible and therefore the</span>
<span class="comment">% classification calculations cause this error to occur.</span>

<span class="comment">% We can remove these neurons in order to ensure a positive definite</span>
<span class="comment">% covariance matrix by setting a threshold on the minimum number of spikes.</span>
<span class="comment">% If a neuron does not spike above the threshold across all trials, it must</span>
<span class="comment">% be removed. Furthermore, if one row is removed from a class, it must</span>
<span class="comment">% be removed from all other classes in order to preserve the dimensions.</span>
</pre><h2>Part C: Model (ii) Classification (Removing Neurons)<a name="6"></a></h2><pre class="codeinput"><span class="comment">% Model (ii) Gaussian, Class Specific Covariance</span>

<span class="comment">% Calculate the total number of spikes per neuron for all of the classes</span>
n_spikes_total = zeros(n_neurons, n_class);
<span class="keyword">for</span> i = 1:n_class
    <span class="comment">% sum the number of spikes for each neuron across the entire train</span>
    n_spikes_class = sum(n_spikes_train{1,i},2);
    <span class="comment">% sum the number of spikes for each neuron across all trials</span>
    n_spikes_total(:,i) = n_spikes_class;
<span class="keyword">end</span>

<span class="comment">% set the minimum spike threshold number to be 10</span>
<span class="comment">% find the neurons which do not spike at least 10 times in each class, and</span>
<span class="comment">% find the unique rows</span>
<span class="comment">% the accuracy increases slightly with higher threshold</span>
thres = 10;
[row,col] = find(n_spikes_total&lt;=thres);
<span class="comment">% find the unique rows that fall below the threshold, so they can be</span>
<span class="comment">% removed from all classes</span>
row_del = unique(row);

<span class="comment">% create a second spike train matrix to store the neurons above the</span>
<span class="comment">% threshold</span>
n_spikes_train_ii = n_spikes_train;
n_spikes_test_ii = n_spikes_test;

<span class="comment">% calculate the total number of neurons left after thresholding</span>
n_neurons_ii = n_neurons-length(row_del);
n_spikes_total_ii = zeros(n_neurons_ii, n_class);

<span class="keyword">for</span> i = 1:n_class
    <span class="comment">% remove the below-threshold neurons across all classes</span>
    n_spikes_train_ii{1,i}(row_del,:) = [];
    n_spikes_test_ii{1,i}(row_del,:) = [];

    <span class="comment">% check that removing these neurons worked</span>
    n_spikes_class_ii = sum(n_spikes_train_ii{1,i},2);
    n_spikes_total_ii(:,i) = n_spikes_class_ii;

<span class="keyword">end</span>

<span class="comment">% preallocate ML parameters</span>
mu_ii = zeros(n_neurons_ii, n_class);
S_k_ii = cell(1, n_class);
sigma_ii = zeros(n_neurons_ii, n_neurons_ii);

<span class="comment">% calculate ML parameters (same as above)</span>
<span class="keyword">for</span> i = 1:n_class
    mu_ii(:,i) = 1/(N_k)*sum(n_spikes_train_ii{1,i},2);
    cov_trial_ii = zeros(n_neurons_ii, n_neurons_ii);
    <span class="keyword">for</span> j = 1:n_trial
        cov_trial_ii = cov_trial_ii + (n_spikes_train_ii{1,i}(:,j)-mu_ii(:,i))<span class="keyword">...</span>
            *(n_spikes_train_ii{1,i}(:,j)-mu_ii(:,i))';
        <span class="comment">% sum the (x-mu)*(x-mu)' matrices for each trial</span>
    <span class="keyword">end</span>
    S_k_ii{i} = 1/N_k * cov_trial_ii;
    <span class="comment">% calculate the S_k for each class and store into cell</span>
<span class="keyword">end</span>

k_ii = zeros(n_trial, n_class);
errors_ii = zeros(1,n_class);

<span class="comment">% classification (same as above)</span>
<span class="keyword">for</span> n = 1:n_class
    xy = n_spikes_test_ii{n}';
    <span class="keyword">for</span> j = 1:n_trial
        <span class="keyword">for</span> i = 1:n_class
            k_ii(j,i) = log(P_Ck)+ mu_ii(:,i)'*inv(S_k_ii{i})*xy(j,:)'-0.5*mu_ii(:,i)'*<span class="keyword">...</span><span class="comment">.</span>
                inv(S_k_ii{i})*mu_ii(:,i) - 0.5*xy(j,:)*inv(S_k_ii{i})*xy(j,:)'-0.5*log(det(S_k_ii{i}));
        <span class="keyword">end</span>
    <span class="keyword">end</span>
    [m_ii,idx_ii] = max(k_ii, [], 2);
    <span class="comment">% classify with max argument of decision function</span>
    errors_ii(n)= length(idx_ii(idx_ii~=n));
    <span class="comment">% save number of incorrect classifications</span>
<span class="keyword">end</span>
accuracy_ii = (1-sum(errors_ii)/(n_trial*n_class))*100;

fprintf(<span class="string">'Model (ii) Gaussian, Class-Specific Covariance Accuracy in percentage:'</span>)
disp(accuracy_ii)
<span class="comment">% report accuracy</span>

<span class="comment">% The accuracy for Model (ii) is lower than for Model (i). In theory, this</span>
<span class="comment">% should be higher but this result is likely due to overfitting. The</span>
<span class="comment">% classifier weighs the activity of the low-firing rate neurons too much,</span>
<span class="comment">% assigning significance to their activity because it is rare. However,</span>
<span class="comment">% since these neurons are most likely noise sources, this decreases the</span>
<span class="comment">% accuracy of the classifier.</span>

<span class="comment">% This explanation is consistent with the observation that accuracy</span>
<span class="comment">% increases slightly when a higher threshold is set, meaning that more of</span>
<span class="comment">% these neurons are notincluded for training.</span>
</pre><pre class="codeoutput">Model (ii) Gaussian, Class-Specific Covariance Accuracy in percentage:   53.7088

</pre><h2>Part D: Naive Bayes<a name="7"></a></h2><pre class="codeinput"><span class="comment">% Model (iii) Poisson</span>

k_iii = zeros(n_trial, n_class);
errors_iii = zeros(1,n_class);

<span class="keyword">for</span> n = 1:n_class
    xy = n_spikes_test_ii{n}';
    <span class="keyword">for</span> j = 1:n_trial
        <span class="keyword">for</span> i = 1:n_class
            k_iii(j,i) =  xy(j,:)*log(mu_ii(:,i)) - sum(mu_ii(:,i)') ;
            <span class="comment">% decision function for Poisson model</span>
        <span class="keyword">end</span>
    <span class="keyword">end</span>
    [m_iii,idx_y(:,n)] = max(k_iii, [], 2);
    <span class="comment">% find max argument for classification</span>
    idx_iii = idx_y(:,n);
    errors_iii(n)= length(idx_iii(idx_iii~=n));
<span class="keyword">end</span>

accuracy_iii = (1-sum(errors_iii)/(n_trial*n_class))*100;
fprintf(<span class="string">'Model (iii) Poisson (Naive Bayes) Accuracy in percentage:'</span>)
disp(accuracy_iii)
</pre><pre class="codeoutput">Model (iii) Poisson (Naive Bayes) Accuracy in percentage:   89.2857

</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2016a</a><br></p></div><!--
##### SOURCE BEGIN #####
% EE239AS Homework 4

clc
clear
close all

%% Problem 4: Real Neural Data

ps4_data = importdata('ps4_realdata.mat');

%% Part A: ML Parameters

train_data = ps4_data.train_trial;
test_data = ps4_data.test_trial;
% since sizes of test and train data sets are the same, extract number of
% spikes in the same loop

n_neurons = 97;           % number of neurons per trial
n_class = size(train_data,2);       % number of classes
n_trial = size(train_data,1);       % number of trials
n_spikes_train = cell(1, n_class);          
% number of spikes per spike train (train data)
n_spikes_test = n_spikes_train;             
% number of spikes per spike train (test data)

% sum the total number of spikes per trial from spike train data

for i = 1:n_trial
    for j = 1:n_class
        n_spikes_train{1,j} = [n_spikes_train{1,j}, sum(train_data(i,j).spikes(:,351:550),2)];
        n_spikes_test{1,j} = [n_spikes_test{1,j}, sum(test_data(i,j).spikes(:,351:550),2)]; 
        % only include the spikes in the middle of the trial due to
        % experimental procedure
    end
end

%% Part A: Model(i) Classification

% Model (i) Gaussian, Shared Covariance

N_k = n_trial;      % number of neurons per class is equal for all classes
N = N_k*n_class;    % total number of neurons
P_Ck = N_k/(n_class*N_k);
% prior probabilities of each class (equal for all classes)

mu_i = zeros(n_neurons, n_class);
S_k_i = cell(1, n_class);
sigma_i = zeros(n_neurons, n_neurons);
% preallocate ML parameter vectors

% calculating ML parameters for classification boundaries
for i = 1:n_class
    mu_i(:,i) = 1/(N_k)*sum(n_spikes_train{1,i},2);
    % calculate mean using results of problem 1
    cov_trial_i = zeros(n_neurons, n_neurons);
    % (x-mu)*(x-mu)' matrices for each trial
    for j = 1:n_trial
        cov_trial_i = cov_trial_i + (n_spikes_train{1,i}(:,j)-mu_i(:,i))*(n_spikes_train{1,i}(:,j)-mu_i(:,i))';
        % sum the (x-mu)*(x-mu)' matrices for each trial 
    end
    
    S_k_i{i} = 1/N_k * cov_trial_i;
    % calculate the S_k for each class and store into cell
    sigma_i = sigma_i + N_k/N * S_k_i{i};
    % calculate sigma (weighted sum of S_k)
end

k = zeros(n_trial, n_class);
errors = zeros(1,n_class);
% preallocate the class prediction and error vectors

% classification of test data
for n = 1:n_class
    xy = n_spikes_test{n}';
    % matrix of neuron firing rates across all trials (97x91) transposed
    for j = 1:n_trial
        for i = 1:n_class
            k(j,i) = log(P_Ck)+ mu_i(:,i)'*inv(sigma_i)*xy(j,:)'-0.5*mu_i(:,i)'...
                *inv(sigma_i)*mu_i(:,i);
            % decision function for each neuron a_k(x)
        end
    end
    [m,idx] = max(k, [], 2);
    % take the maximum of this to decide which class the neuron belongs in
    errors(n)= length(idx(idx~=n));     
    % number of incorrect classifications (not matching the specified reach
    % angle)
end
accuracy = (1-sum(errors)/(n_trial*n_class))*100;
fprintf('Model (i) Gaussian, Shared Covariance Accuracy in percentage:')
disp(accuracy)
% calculate accuracy of classification

%% Part B: Model (ii) Classification

% Model (ii) Gaussian, Class Specific Covariance

% The code above was run using the ML parameters for Model (ii) (same mu,
% and class-specific sigmas). However, the error:
 
% Matrix is close to singular or badly scaled. Results may be inaccurate.

% occurs. This happens because there are neurons that do not fire very much 
% across all the trials, causing the covariance matrix to be close to
% singular (due to the determinant being very close to zero). When the
% covariance matrix is singular, it is not invertible and therefore the
% classification calculations cause this error to occur. 

% We can remove these neurons in order to ensure a positive definite
% covariance matrix by setting a threshold on the minimum number of spikes.
% If a neuron does not spike above the threshold across all trials, it must
% be removed. Furthermore, if one row is removed from a class, it must
% be removed from all other classes in order to preserve the dimensions. 

%% Part C: Model (ii) Classification (Removing Neurons)

% Model (ii) Gaussian, Class Specific Covariance

% Calculate the total number of spikes per neuron for all of the classes
n_spikes_total = zeros(n_neurons, n_class);
for i = 1:n_class
    % sum the number of spikes for each neuron across the entire train
    n_spikes_class = sum(n_spikes_train{1,i},2);
    % sum the number of spikes for each neuron across all trials
    n_spikes_total(:,i) = n_spikes_class;
end

% set the minimum spike threshold number to be 10
% find the neurons which do not spike at least 10 times in each class, and
% find the unique rows
% the accuracy increases slightly with higher threshold
thres = 10;
[row,col] = find(n_spikes_total<=thres);
% find the unique rows that fall below the threshold, so they can be
% removed from all classes
row_del = unique(row);

% create a second spike train matrix to store the neurons above the
% threshold
n_spikes_train_ii = n_spikes_train;
n_spikes_test_ii = n_spikes_test;

% calculate the total number of neurons left after thresholding
n_neurons_ii = n_neurons-length(row_del);
n_spikes_total_ii = zeros(n_neurons_ii, n_class);

for i = 1:n_class
    % remove the below-threshold neurons across all classes
    n_spikes_train_ii{1,i}(row_del,:) = [];
    n_spikes_test_ii{1,i}(row_del,:) = [];

    % check that removing these neurons worked
    n_spikes_class_ii = sum(n_spikes_train_ii{1,i},2);
    n_spikes_total_ii(:,i) = n_spikes_class_ii;

end

% preallocate ML parameters
mu_ii = zeros(n_neurons_ii, n_class);
S_k_ii = cell(1, n_class);
sigma_ii = zeros(n_neurons_ii, n_neurons_ii);

% calculate ML parameters (same as above)
for i = 1:n_class
    mu_ii(:,i) = 1/(N_k)*sum(n_spikes_train_ii{1,i},2);
    cov_trial_ii = zeros(n_neurons_ii, n_neurons_ii);
    for j = 1:n_trial
        cov_trial_ii = cov_trial_ii + (n_spikes_train_ii{1,i}(:,j)-mu_ii(:,i))...
            *(n_spikes_train_ii{1,i}(:,j)-mu_ii(:,i))';
        % sum the (x-mu)*(x-mu)' matrices for each trial 
    end
    S_k_ii{i} = 1/N_k * cov_trial_ii;
    % calculate the S_k for each class and store into cell
end

k_ii = zeros(n_trial, n_class);
errors_ii = zeros(1,n_class);

% classification (same as above)
for n = 1:n_class
    xy = n_spikes_test_ii{n}';
    for j = 1:n_trial
        for i = 1:n_class
            k_ii(j,i) = log(P_Ck)+ mu_ii(:,i)'*inv(S_k_ii{i})*xy(j,:)'-0.5*mu_ii(:,i)'*....
                inv(S_k_ii{i})*mu_ii(:,i) - 0.5*xy(j,:)*inv(S_k_ii{i})*xy(j,:)'-0.5*log(det(S_k_ii{i}));
        end
    end
    [m_ii,idx_ii] = max(k_ii, [], 2);
    % classify with max argument of decision function
    errors_ii(n)= length(idx_ii(idx_ii~=n));
    % save number of incorrect classifications
end
accuracy_ii = (1-sum(errors_ii)/(n_trial*n_class))*100;

fprintf('Model (ii) Gaussian, Class-Specific Covariance Accuracy in percentage:')
disp(accuracy_ii)
% report accuracy

% The accuracy for Model (ii) is lower than for Model (i). In theory, this
% should be higher but this result is likely due to overfitting. The
% classifier weighs the activity of the low-firing rate neurons too much,
% assigning significance to their activity because it is rare. However,
% since these neurons are most likely noise sources, this decreases the
% accuracy of the classifier. 

% This explanation is consistent with the observation that accuracy 
% increases slightly when a higher threshold is set, meaning that more of 
% these neurons are notincluded for training. 


%% Part D: Naive Bayes

% Model (iii) Poisson

k_iii = zeros(n_trial, n_class);
errors_iii = zeros(1,n_class);

for n = 1:n_class
    xy = n_spikes_test_ii{n}';
    for j = 1:n_trial   
        for i = 1:n_class
            k_iii(j,i) =  xy(j,:)*log(mu_ii(:,i)) - sum(mu_ii(:,i)') ;
            % decision function for Poisson model
        end
    end
    [m_iii,idx_y(:,n)] = max(k_iii, [], 2);
    % find max argument for classification
    idx_iii = idx_y(:,n);
    errors_iii(n)= length(idx_iii(idx_iii~=n));
end

accuracy_iii = (1-sum(errors_iii)/(n_trial*n_class))*100;
fprintf('Model (iii) Poisson (Naive Bayes) Accuracy in percentage:')
disp(accuracy_iii)

##### SOURCE END #####
--></body></html>